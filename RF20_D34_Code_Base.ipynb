{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac323e6",
   "metadata": {},
   "source": [
    "# Replication Code for Deliverable 3.4\n",
    "\n",
    "## Overview\n",
    "\n",
    "The following code is provided for the replication of the results of Deliverable 3.4 (Extended Energy Management with Forecasting of Renewable Generation) of Horizon Europe project Research Facility 2.0 ([https://rf20.eu/](https://rf20.eu/)).\n",
    "\n",
    "## Code Structure\n",
    "\n",
    "The code is divided into three main parts following the structure of report D3.4:\n",
    "\n",
    "1. **TIME SERIES FORECASTING OF RENEWABLE GENERATION**\n",
    "2. **EXTENDED ENERGY MANAGEMENT** (with Model Predictive Control)\n",
    "3. **LONG-TERM RENEWABLE ENERGY SCENARIO GENERATION**\n",
    "\n",
    "## Data Availability and Replication Limitations\n",
    "\n",
    "Exact replication of the results, particularly for Section 2 (EXTENDED ENERGY MANAGEMENT), will be difficult as the following proprietary input data was used:\n",
    "\n",
    "1. Real load demand profile of the accelerator\n",
    "2. The grid tariff structure of the accelerator electrical energy supply contract\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53bf6e",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets\n",
    "'KARA_PVSF_featured_extended.csv'\n",
    "   # USER MUST SPECIFY: Path to SSP climate data directory\n",
    "    data_path = './climate_data'  # Modify this path solardf['PV']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea152bfa",
   "metadata": {},
   "source": [
    "# 1. TIME SERIES FORECASTING OF RENEWABLE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f12db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Time Series Forecasting for PV Generation using AutoML Neural Networks\n",
    "\n",
    "This module implements automated hyperparameter tuning and model training for \n",
    "photovoltaic (PV) power generation forecasting using LSTM neural network \n",
    "architectures from the NeuralForecast library.\n",
    "\n",
    "Author: Malik Muhammad Abdullah @ Karlsruhe Institute of Technology(Research Facility 2.0 Project)\n",
    "License: European Union Public Licence (EUPL) v1.2 or later\n",
    "Copyright: © 2025 Research Facility 2.0 Consortium\n",
    "\"\"\"\n",
    "# Copyright © 2025 Research Facility 2.0 Consortium\n",
    "#\n",
    "# Licensed under the EUPL, Version 1.2 or – as soon they will be approved by\n",
    "# the European Commission - subsequent versions of the EUPL (the \"Licence\");\n",
    "# You may not use this work except in compliance with the Licence.\n",
    "# You may obtain a copy of the Licence at:\n",
    "#\n",
    "# https://joinup.ec.europa.eu/collection/eupl/eupl-text-eupl-12\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the Licence is distributed on an \"AS IS\" basis,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the Licence for the specific language governing permissions and\n",
    "# limitations under the Licence.\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "\n",
    "from neuralforecast.losses.pytorch import DistributionLoss, MAE, MQLoss, RMSE\n",
    "from neuralforecast.auto import AutoLSTM\n",
    "  \n",
    "from neuralforecast.core import NeuralForecast\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Suppress verbose logging from PyTorch Lightning and Optuna\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Clear CUDA cache to prevent memory issues\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_preprocess_data(filepath='KARA_PVSF_featured_extended.csv'):\n",
    "    \"\"\"\n",
    "    Load PV generation data with engineered features and prepare for modeling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the CSV file containing PV generation data with features\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Preprocessed dataframe with forward-filled missing values\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    - Renames 'pv_duty_factor' to 'y' as the target variable\n",
    "    - Drops the original 'y' column if present\n",
    "    - Forward fills any missing values to maintain temporal continuity\n",
    "    \"\"\"\n",
    "    # Load the featured dataset\n",
    "    df_pv1 = pd.read_csv(filepath)\n",
    "    \n",
    "    # Drop original target column and rename pv_duty_factor as the new target\n",
    "    df_pv1 = df_pv1.drop(['y'], axis=1)\n",
    "    df_pv1 = df_pv1.rename(columns={'pv_duty_factor': 'y'})\n",
    "    \n",
    "    # Forward fill missing values to maintain temporal continuity\n",
    "    df_pv1 = df_pv1.fillna(method='ffill')\n",
    "    \n",
    "    return df_pv1\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df_pv1 = load_and_preprocess_data('KARA_PVSF_featured_extended.csv')\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Selected features based on feature importance analysis or domain knowledge\n",
    "# Features include:\n",
    "# - Lag features: Previous timestep values (lag_1, lag_4, lag_96, lag_672)\n",
    "# - Moving averages: EMA (4, 12, 24), WMA trends (72-168 hours)\n",
    "# - Rolling statistics: Mean, max, min over 3h and 5h windows\n",
    "# - Momentum and trend: Differences and momentum indicators\n",
    "# - Seasonal features: Z-scores and ratios at different seasonal periods\n",
    "# - Irradiance features: Global tilted, clear sky (DNI, DHI, GHI)\n",
    "# - Weather features: Temperature, humidity, sunshine duration\n",
    "# - Solar geometry: Altitude, zenith, incidence angles\n",
    "\n",
    "selected_feature_names = [\n",
    "    # EMA (Exponential Moving Average) features\n",
    "    'pv_duty_factor_ema_4', 'y_ema_4',\n",
    "    'pv_duty_factor_ema_12', 'y_ema_12',\n",
    "    'pv_duty_factor_ema_24', 'y_ema_24',\n",
    "    \n",
    "    # Lag features (autoregressive components)\n",
    "    'y_lag_1', 'pv_duty_factor_lag_1',\n",
    "    'y_lag_4', 'pv_duty_factor_lag_4',\n",
    "    'y_lag_96', 'pv_duty_factor_lag_96',\n",
    "    'y_lag_672', 'pv_duty_factor_lag_672',\n",
    "    \n",
    "    # Irradiance features\n",
    "    'global_tilted_irradiance',\n",
    "    'global_tilted_irradiance_instant',\n",
    "    'clear_sky_dni', 'clear_sky_dhi', 'clear_sky_ghi',\n",
    "    'clear_sky_ratio',\n",
    "    \n",
    "    # Rolling window statistics\n",
    "    'y_rolling_mean_3h', 'pv_duty_factor_rolling_mean_3h',\n",
    "    'y_rolling_max_3h', 'pv_duty_factor_rolling_max_3h',\n",
    "    'y_rolling_min_3h', 'pv_duty_factor_rolling_min_3h',\n",
    "    'y_rolling_mean_5h', 'pv_duty_factor_rolling_mean_5h',\n",
    "    'y_rolling_max_5h', 'pv_duty_factor_rolling_max_5h',\n",
    "    \n",
    "    # WMA (Weighted Moving Average) trend features\n",
    "    'pv_duty_factor_wma_trend_72', 'y_wma_trend_72',\n",
    "    'pv_duty_factor_wma_trend_96', 'y_wma_trend_96',\n",
    "    'pv_duty_factor_wma_trend_120', 'y_wma_trend_120',\n",
    "    'pv_duty_factor_wma_trend_144', 'y_wma_trend_144',\n",
    "    'pv_duty_factor_wma_trend_168', 'y_wma_trend_168',\n",
    "    \n",
    "    # EMA trend features\n",
    "    'pv_duty_factor_ema_trend_19', 'y_ema_trend_19',\n",
    "    'pv_duty_factor_ema_trend_21', 'y_ema_trend_21',\n",
    "    'pv_duty_factor_ema_trend_24', 'y_ema_trend_24',\n",
    "    'pv_duty_factor_ema_trend_48', 'y_ema_trend_48',\n",
    "    'pv_duty_factor_ema_trend_72', 'y_ema_trend_72',\n",
    "    \n",
    "    # Momentum and difference features\n",
    "    'pv_duty_factor_momentum_48', 'y_momentum_48',\n",
    "    'pv_duty_factor_diff_trend_48', 'y_diff_trend_48',\n",
    "    'pv_duty_factor_momentum_144', 'y_momentum_144',\n",
    "    'pv_duty_factor_diff_trend_144', 'y_diff_trend_144',\n",
    "    'pv_duty_factor_momentum_168', 'y_momentum_168',\n",
    "    'pv_duty_factor_diff_trend_168', 'y_diff_trend_168',\n",
    "    \n",
    "    # Seasonal features\n",
    "    'pv_duty_factor_seasonal_zscore_96', 'y_seasonal_zscore_96',\n",
    "    'pv_duty_factor_seasonal_zscore_672', 'y_seasonal_zscore_672',\n",
    "    'pv_duty_factor_seasonal_ratio_672', 'y_seasonal_ratio_672',\n",
    "    \n",
    "    # Weather and environmental features\n",
    "    'Tc(C)',  # Cell temperature\n",
    "    'relative_humidity_2m',\n",
    "    'sunshine_duration', 'sunshine_duration.1',\n",
    "    \n",
    "    # Solar geometry\n",
    "    'altitude', 'zenith', 'incidence',\n",
    "    \n",
    "    # Timestamp and target\n",
    "    'ds',  # datetime\n",
    "    'y'    # target variable\n",
    "]\n",
    "\n",
    "# Select only the chosen features\n",
    "df_selected = df_pv1[selected_feature_names]\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN/VALIDATION/TEST SPLIT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_forecasting_dataframe(df, unique_id=1):\n",
    "    \"\"\"\n",
    "    Prepare dataframe for NeuralForecast format with proper datetime handling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with features and target\n",
    "    unique_id : int, optional\n",
    "        Identifier for the time series (default=1)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Formatted dataframe ready for NeuralForecast\n",
    "    \"\"\"\n",
    "    Y_df = df.copy()\n",
    "    Y_df['unique_id'] = unique_id\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "    return Y_df\n",
    "\n",
    "\n",
    "# Prepare the dataframe\n",
    "Y_df2 = prepare_forecasting_dataframe(df_selected)\n",
    "\n",
    "# Calculate train/validation/test split sizes\n",
    "# Using 15% for validation and 15% for test (70% for training)\n",
    "n_time = len(Y_df2.ds.unique())\n",
    "val_size = int(0.15 * n_time)   # 15% validation\n",
    "test_size = int(0.15 * n_time)  # 15% test\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETER SEARCH CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def config_autoLSTM(trial):\n",
    "    \"\"\"\n",
    "    Define hyperparameter search space for AutoLSTM model using Optuna.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object for hyperparameter suggestion\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of hyperparameters for the LSTM model\n",
    "        \n",
    "    Hyperparameters\n",
    "    ---------------\n",
    "    input_size : int\n",
    "        Lookback window size (-1 for automatic selection)\n",
    "    encoder_hidden_size : int\n",
    "        Number of hidden units in encoder LSTM layers\n",
    "    encoder_n_layers : int\n",
    "        Number of stacked LSTM layers in encoder\n",
    "    context_size : int\n",
    "        Size of context vector for decoder\n",
    "    decoder_hidden_size : int\n",
    "        Number of hidden units in decoder LSTM layers\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer (log-scale search)\n",
    "    max_steps : int\n",
    "        Maximum training steps\n",
    "    batch_size : int\n",
    "        Number of samples per batch\n",
    "    \"\"\"\n",
    "    return {\n",
    "        # Lookback window: -1 = automatic, or fixed values\n",
    "        \"input_size\": trial.suggest_categorical(\n",
    "            \"input_size\", [-1, 4, 16, 64, 96, 168]\n",
    "        ),\n",
    "        \"inference_input_size\": -1,  # Use same as training\n",
    "        \"h\": None,  # Horizon set externally\n",
    "        \n",
    "        # Encoder architecture\n",
    "        \"encoder_hidden_size\": trial.suggest_categorical(\n",
    "            \"encoder_hidden_size\", [16, 32, 64, 128]\n",
    "        ),\n",
    "        \"encoder_n_layers\": trial.suggest_int(\"encoder_n_layers\", 1, 4),\n",
    "        \n",
    "        # Context representation\n",
    "        \"context_size\": trial.suggest_categorical(\"context_size\", [5, 10, 50]),\n",
    "        \n",
    "        # Decoder architecture\n",
    "        \"decoder_hidden_size\": trial.suggest_categorical(\n",
    "            \"decoder_hidden_size\", [16, 32, 64, 128]\n",
    "        ),\n",
    "        \n",
    "        # Training configuration\n",
    "        \"learning_rate\": trial.suggest_float(\n",
    "            \"learning_rate\", 1e-4, 1e-1, log=True\n",
    "        ),\n",
    "        \"max_steps\": trial.suggest_categorical(\"max_steps\", [500, 10000]),\n",
    "        \"batch_size\": trial.suggest_categorical(\n",
    "            \"batch_size\", [32, 64, 128, 256, 512]\n",
    "        ),\n",
    "        \n",
    "        # Reproducibility\n",
    "        \"random_seed\": 1,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Forecasting horizon: 96 timesteps (15-min intervals = 24 hours ahead)\n",
    "horizon = 96\n",
    "\n",
    "# Number of Optuna hyperparameter search trials\n",
    "num_samples = 25\n",
    "\n",
    "# Initialize models list with AutoLSTM\n",
    "\n",
    "# but can be activated by uncommenting the respective blocks\n",
    "models = [\n",
    "    AutoLSTM(\n",
    "        h=horizon,  # Forecast horizon\n",
    "        backend=\"optuna\",  # Use Optuna for hyperparameter optimization\n",
    "        loss=MQLoss(level=[80]),  # Quantile loss at 80th percentile\n",
    "        valid_loss=MQLoss(level=[80]),  # Validation uses same loss\n",
    "        config=config_autoLSTM,  # Custom hyperparameter search space\n",
    "        num_samples=num_samples  # Number of optimization trials\n",
    "    ),\n",
    "    \n",
    "   \n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL TRAINING AND CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize NeuralForecast with model configuration\n",
    "nf_LSTM = NeuralForecast(\n",
    "    models=models,\n",
    "    freq='15min',  # Data frequency: 15-minute intervals\n",
    "    local_scaler_type='standard'  # Standardize features (z-score normalization)\n",
    ")\n",
    "\n",
    "# Perform cross-validation with automatic train/val/test splitting\n",
    "# This will train the model and generate predictions on validation and test sets\n",
    "pred_df_LSTM = nf_LSTM.cross_validation(\n",
    "    df=Y_df2,\n",
    "    val_size=val_size,\n",
    "    test_size=test_size,\n",
    "    n_windows=None  # Use default windowing strategy\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# CUSTOM LOSS FUNCTIONS FOR EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "class ForecastingLosses:\n",
    "    \"\"\"\n",
    "    Comprehensive collection of forecasting evaluation metrics.\n",
    "    \n",
    "    This class provides deterministic loss functions commonly used in \n",
    "    time series forecasting evaluation, including scale-dependent,\n",
    "    scale-independent, and relative error metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize PyTorch loss functions.\"\"\"\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "    \n",
    "    def mse(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Mean Squared Error (MSE).\n",
    "        \n",
    "        Measures average squared difference between predictions and actuals.\n",
    "        Sensitive to outliers due to squaring.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            MSE loss value\n",
    "        \"\"\"\n",
    "        return self.mse_loss(y_pred, y_true)\n",
    "    \n",
    "    def rmse(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Root Mean Squared Error (RMSE).\n",
    "        \n",
    "        Square root of MSE, interpretable in the same units as the target.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            RMSE loss value\n",
    "        \"\"\"\n",
    "        mse = self.mse_loss(y_pred, y_true)\n",
    "        return torch.sqrt(mse)\n",
    "    \n",
    "    def mae(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Mean Absolute Error (MAE).\n",
    "        \n",
    "        Average absolute difference between predictions and actuals.\n",
    "        Less sensitive to outliers than MSE/RMSE.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            MAE loss value\n",
    "        \"\"\"\n",
    "        return self.mae_loss(y_pred, y_true)\n",
    "    \n",
    "    def mape(self, y_true, y_pred, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Mean Absolute Percentage Error (MAPE).\n",
    "        \n",
    "        Average absolute percentage error. Scale-independent but undefined\n",
    "        when true values are zero. Asymmetric (penalizes over-predictions\n",
    "        more than under-predictions).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted values\n",
    "        epsilon : float, optional\n",
    "            Small constant to avoid division by zero (default=1e-8)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            MAPE as a percentage\n",
    "        \"\"\"\n",
    "        ape = torch.abs((y_true - y_pred) / (y_true + epsilon))\n",
    "        return torch.mean(ape) * 100\n",
    "    \n",
    "    def smape(self, y_true, y_pred, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Symmetric Mean Absolute Percentage Error (sMAPE).\n",
    "        \n",
    "        Symmetric variant of MAPE that treats over- and under-predictions\n",
    "        equally. Bounded between 0% and 200%.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted values\n",
    "        epsilon : float, optional\n",
    "            Small constant to avoid division by zero (default=1e-8)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            sMAPE as a percentage\n",
    "        \"\"\"\n",
    "        numerator = torch.abs(y_pred - y_true)\n",
    "        denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2 + epsilon\n",
    "        return torch.mean(numerator / denominator) * 100\n",
    "    \n",
    "    def mase(self, y_true, y_pred, y_train=None, seasonality=1):\n",
    "        \"\"\"\n",
    "        Mean Absolute Scaled Error (MASE).\n",
    "        \n",
    "        Scales MAE by the MAE of a naive seasonal forecast on training data.\n",
    "        Values < 1 indicate better performance than naive forecast.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted values\n",
    "        y_train : torch.Tensor\n",
    "            Training data for computing naive forecast baseline\n",
    "        seasonality : int, optional\n",
    "            Seasonal period for naive forecast (default=1 for non-seasonal)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            MASE value\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If y_train is not provided\n",
    "        \"\"\"\n",
    "        if y_train is None:\n",
    "            raise ValueError(\"MASE requires training data for naive forecast\")\n",
    "        \n",
    "        # Naive forecast: use value from 'seasonality' steps ago\n",
    "        naive_forecast = y_train[:-seasonality]\n",
    "        naive_mae = torch.mean(torch.abs(y_train[seasonality:] - naive_forecast))\n",
    "        forecast_mae = torch.mean(torch.abs(y_true - y_pred))\n",
    "        \n",
    "        return forecast_mae / naive_mae\n",
    "    \n",
    "    def mqloss(self, y_true, y_pred, q=0.8):\n",
    "        \"\"\"\n",
    "        Multiplicative Quantile Loss (MQ Loss).\n",
    "        \n",
    "        Asymmetric loss function that penalizes under-prediction more heavily\n",
    "        when q > 0.5 (useful for risk-averse forecasting).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values, shape [batch]\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted values, shape [batch]\n",
    "        q : float, optional\n",
    "            Quantile level (default=0.8 for 80th percentile)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            MQ loss value\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        For q=0.8, under-predictions are penalized 4x more than over-predictions.\n",
    "        \"\"\"\n",
    "        error = y_true - y_pred\n",
    "        loss = torch.max((q - 1) * error, q * error)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n",
    "class QuantileLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-quantile loss for probabilistic forecasting.\n",
    "    \n",
    "    Computes pinball loss across multiple quantiles simultaneously,\n",
    "    enabling prediction intervals and uncertainty quantification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, quantiles=[0.1, 0.5, 0.9]):\n",
    "        \"\"\"\n",
    "        Initialize multi-quantile loss.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        quantiles : list of float\n",
    "            Quantile levels to compute (e.g., [0.1, 0.5, 0.9] for \n",
    "            10th percentile, median, and 90th percentile)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.quantiles = quantiles\n",
    "    \n",
    "    def forward(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute average pinball loss across all quantiles.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted quantiles, shape [batch, n_quantiles]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Average quantile loss\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for i, q in enumerate(self.quantiles):\n",
    "            errors = y_true - y_pred[:, i]\n",
    "            loss = torch.max((q - 1) * errors, q * errors)\n",
    "            losses.append(torch.mean(loss))\n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "class NormalizedRMSE(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalized Root Mean Squared Error (NRMSE).\n",
    "    \n",
    "    Scales RMSE by a measure of data spread to make it comparable\n",
    "    across different datasets or variables with different scales.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, normalization_method='range', epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Initialize NRMSE with specified normalization method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        normalization_method : str, optional\n",
    "            Method for normalization:\n",
    "            - 'range': Normalize by (max - min)\n",
    "            - 'std': Normalize by standard deviation\n",
    "            - 'mean': Normalize by mean absolute value\n",
    "            - 'iqr': Normalize by interquartile range (Q75 - Q25)\n",
    "        epsilon : float, optional\n",
    "            Small constant to avoid division by zero (default=1e-8)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.normalization_method = normalization_method\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute normalized RMSE.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            NRMSE value\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If unknown normalization method is specified\n",
    "        \"\"\"\n",
    "        # Compute RMSE\n",
    "        mse = nn.MSELoss()(y_pred, y_true)\n",
    "        rmse = torch.sqrt(mse)\n",
    "        \n",
    "        # Calculate normalization factor based on method\n",
    "        if self.normalization_method == 'range':\n",
    "            data_range = torch.max(y_true) - torch.min(y_true)\n",
    "            norm_factor = data_range + self.epsilon\n",
    "        \n",
    "        elif self.normalization_method == 'std':\n",
    "            norm_factor = torch.std(y_true) + self.epsilon\n",
    "        \n",
    "        elif self.normalization_method == 'mean':\n",
    "            norm_factor = torch.mean(torch.abs(y_true)) + self.epsilon\n",
    "        \n",
    "        elif self.normalization_method == 'iqr':\n",
    "            q75 = torch.quantile(y_true, 0.75)\n",
    "            q25 = torch.quantile(y_true, 0.25)\n",
    "            norm_factor = (q75 - q25) + self.epsilon\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown normalization method: {self.normalization_method}\"\n",
    "            )\n",
    "        \n",
    "        # Return normalized RMSE\n",
    "        normalized_rmse = rmse / norm_factor\n",
    "        return normalized_rmse\n",
    "\n",
    "\n",
    "class NRMSE(nn.Module):\n",
    "    \"\"\"\n",
    "    Alternative implementation of Normalized RMSE with dynamic method selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize NRMSE module.\"\"\"\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y_true, y_pred, method='range'):\n",
    "        \"\"\"\n",
    "        Compute NRMSE with specified normalization method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth values\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted values\n",
    "        method : str, optional\n",
    "            Normalization method ('range', 'mean', 'std', or any other \n",
    "            defaults to no normalization)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            NRMSE value\n",
    "        \"\"\"\n",
    "        rmse = torch.sqrt(nn.MSELoss()(y_pred, y_true))\n",
    "        \n",
    "        if method == 'range':\n",
    "            norm = torch.max(y_true) - torch.min(y_true)\n",
    "        elif method == 'mean':\n",
    "            norm = torch.mean(torch.abs(y_true))\n",
    "        elif method == 'std':\n",
    "            norm = torch.std(y_true)\n",
    "        else:\n",
    "            norm = 1.0\n",
    "        \n",
    "        return rmse / (norm + 1e-8)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE LOSS CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_all_losses(y_true, y_pred, y_train=None):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive set of forecasting evaluation metrics.\n",
    "    \n",
    "    Computes all deterministic and probabilistic metrics for model evaluation,\n",
    "    providing a complete assessment of forecast quality from multiple perspectives.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : torch.Tensor\n",
    "        Ground truth values\n",
    "    y_pred : torch.Tensor\n",
    "        Predicted values\n",
    "    y_train : torch.Tensor, optional\n",
    "        Training data for MASE calculation (default=None)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing all computed loss metrics with keys:\n",
    "        - 'MSE': Mean Squared Error\n",
    "        - 'RMSE': Root Mean Squared Error\n",
    "        - 'MAE': Mean Absolute Error\n",
    "        - 'MAPE': Mean Absolute Percentage Error\n",
    "        - 'sMAPE': Symmetric Mean Absolute Percentage Error\n",
    "        - 'NRMSE_range': Normalized RMSE (range normalization)\n",
    "        - 'NRMSE_std': Normalized RMSE (std normalization)\n",
    "        - 'MASE_24': Mean Absolute Scaled Error (24-step seasonality)\n",
    "        - 'MQ_Loss_q08': Multiplicative Quantile Loss at q=0.8\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    All metrics are returned as Python floats (.item() called on tensors).\n",
    "    MASE_24 is only computed if y_train is provided.\n",
    "    \"\"\"\n",
    "    losses = {}\n",
    "    \n",
    "    # Initialize loss function objects\n",
    "    base_losses = ForecastingLosses()\n",
    "    nrmse_range = NormalizedRMSE(normalization_method='range')\n",
    "    nrmse_std = NormalizedRMSE(normalization_method='std')\n",
    "    quantile_loss = QuantileLoss(quantiles=[0.1, 0.5, 0.9])\n",
    "    \n",
    "    # ===== Basic deterministic losses =====\n",
    "    losses['MSE'] = base_losses.mse(y_true, y_pred).item()\n",
    "    losses['RMSE'] = base_losses.rmse(y_true, y_pred).item()\n",
    "    losses['MAE'] = base_losses.mae(y_true, y_pred).item()\n",
    "    losses['MAPE'] = base_losses.mape(y_true, y_pred).item()\n",
    "    losses['sMAPE'] = base_losses.smape(y_true, y_pred).item()\n",
    "    \n",
    "    # ===== Normalized RMSE variants =====\n",
    "    losses['NRMSE_range'] = nrmse_range(y_true, y_pred).item()\n",
    "    losses['NRMSE_std'] = nrmse_std(y_true, y_pred).item()\n",
    "    \n",
    "    # ===== MASE (if training series available) =====\n",
    "    # Using 24-step seasonality (24 hours for 15-min data = 96 steps, but using 24 for daily pattern)\n",
    "    if y_train is not None:\n",
    "        losses['MASE_24'] = base_losses.mase(\n",
    "            y_true, y_pred, y_train, seasonality=24\n",
    "        ).item()\n",
    "    \n",
    "    # ===== Multiplicative Quantile Loss for 80th percentile =====\n",
    "    # Useful for risk-averse forecasting where under-prediction is costly\n",
    "    losses['MQ_Loss_q08'] = base_losses.mqloss(\n",
    "        y_true.squeeze(), \n",
    "        y_pred.squeeze(), \n",
    "        q=0.8\n",
    "        ).item()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION ON PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Convert prediction results to PyTorch tensors for loss calculation\n",
    "y_true = torch.tensor(\n",
    "    pred_df_LSTM[['y']].values, \n",
    "    dtype=torch.float32\n",
    ")  # Actual PV generation values\n",
    "\n",
    "y_pred = torch.tensor(\n",
    "    pred_df_LSTM[['AutoLSTM-median']].values, \n",
    "    dtype=torch.float32\n",
    ")  # Median predictions from AutoLSTM\n",
    "\n",
    "# Calculate all evaluation metrics\n",
    "# Note: y_train not provided, so MASE will be skipped\n",
    "results = calculate_all_losses(y_true, y_pred, y_train=None)\n",
    "\n",
    "# Display results\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a17fa",
   "metadata": {},
   "source": [
    "# 2. EXTENDED ENERGY MANAGEMENT(with Model Predictive Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Predictive Control (MPC) for Multi-Battery Energy Storage Systems\n",
    "\n",
    "This module implements a receding-horizon optimal control strategy for managing\n",
    "heterogeneous battery energy storage fleets under dynamic electricity tariffs.\n",
    "The optimization jointly minimizes energy costs, peak demand charges, and battery\n",
    "degradation while respecting physical constraints and solar curtailment policies.\n",
    "\n",
    "Features:\n",
    "- Multi-group battery fleet management \n",
    "- Fixed electricity tariff structure (single-tier pricing)\n",
    "- Solar curtailment and power dumping logic\n",
    "- Adaptive terminal SOC targets based on forecast peak severity\n",
    "- Gurobi (commercial) or CLARABEL (open-source) solver support\n",
    "\n",
    "Author: Malik Muhammad Abdullah @ Karlsruhe Institute of Technology(Research Facility 2.0 Project)\n",
    "License: European Union Public Licence (EUPL) v1.2 or later\n",
    "Copyright: © 2025 Research Facility 2.0 Consortium\n",
    "\"\"\"\n",
    "\n",
    "# Copyright © 2025 Research Facility 2.0 Consortium\n",
    "#\n",
    "# Licensed under the EUPL, Version 1.2 or – as soon they will be approved by\n",
    "# the European Commission - subsequent versions of the EUPL (the \"Licence\");\n",
    "# You may not use this work except in compliance with the Licence.\n",
    "# You may obtain a copy of the Licence at:\n",
    "#\n",
    "# https://joinup.ec.europa.eu/collection/eupl/eupl-text-eupl-12\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the Licence is distributed on an \"AS IS\" basis,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the Licence for the specific language governing permissions and\n",
    "# limitations under the Licence.\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "\n",
    "# =============================================================================\n",
    "# SOLVER CONFIGURATION\n",
    "# =============================================================================\n",
    "# Note: This code supports both Gurobi (commercial) and CLARABEL (open-source).\n",
    "# For Gurobi, ensure you have a valid license (academic, commercial, or WLS).\n",
    "# Set your Gurobi license via environment variables or gurobipy configuration.\n",
    "# For open-source alternative, set use_gurobi=False to use CLARABEL.\n",
    "\n",
    "# =============================================================================\n",
    "# BATTERY FLEET CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def build_battery_fleet(battery_params):\n",
    "    \"\"\"\n",
    "    Convert battery parameters into standardized fleet format.\n",
    "    \n",
    "    Supports legacy single-technology format with automatic conversion to\n",
    "    fleet-based representation for consistent internal processing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    battery_params : dict\n",
    "        Battery configuration with keys:\n",
    "        - 'capacity_per_battery': float, capacity per unit (MWh)\n",
    "        - 'max_power_per_battery': float, power rating per unit (MW)\n",
    "        - 'efficiency': float, round-trip efficiency (charging * discharging)\n",
    "        - 'soc_min': float, minimum SOC (fraction, e.g., 0.05)\n",
    "        - 'soc_max': float, maximum SOC (fraction, e.g., 0.95)\n",
    "        - 'battery_type': str, technology type (e.g., 'LFP', 'NMC')\n",
    "        - 'n_units': int, number of battery units (default=1)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Standardized fleet specification with:\n",
    "        - 'n_groups': int, number of battery groups\n",
    "        - 'cap_mwh': np.ndarray, total capacity per group (MWh)\n",
    "        - 'pmax_mw': np.ndarray, total power per group (MW)\n",
    "        - 'eta_ch': np.ndarray, charging efficiency per group\n",
    "        - 'eta_dc': np.ndarray, discharging efficiency per group\n",
    "        - 'soc_min': np.ndarray, min SOC per group\n",
    "        - 'soc_max': np.ndarray, max SOC per group\n",
    "        - 'names': list of str, group identifiers\n",
    "        \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    n_units = int(battery_params.get('n_units', 1))\n",
    "    \n",
    "    # Handle capacity units (support both MWh and kWh inputs)\n",
    "    cap_per_unit = float(battery_params['capacity_per_battery'])\n",
    "    if cap_per_unit < 10:  # Assume MWh if < 10\n",
    "        cap_mwh_u = cap_per_unit\n",
    "    else:  # Assume kWh if >= 10\n",
    "        cap_mwh_u = cap_per_unit / 1000.0\n",
    "    \n",
    "    # Handle power units (support both MW and kW inputs)\n",
    "    p_per_unit = float(battery_params['max_power_per_battery'])\n",
    "    if p_per_unit < 10:  # Assume MW if < 10\n",
    "        p_mw_u = p_per_unit\n",
    "    else:  # Assume kW if >= 10\n",
    "        p_mw_u = p_per_unit / 1000.0\n",
    "    \n",
    "    # Efficiency and SOC limits\n",
    "    eta = float(battery_params['efficiency'])\n",
    "    soc_min = float(battery_params['soc_min'])\n",
    "    soc_max = float(battery_params['soc_max'])\n",
    "    tech = battery_params.get('battery_type', 'BESS')\n",
    "    \n",
    "    # Aggregate all units into single group\n",
    "    return {\n",
    "        'n_groups': 1,\n",
    "        'cap_mwh': np.array([n_units * cap_mwh_u]),\n",
    "        'pmax_mw': np.array([n_units * p_mw_u]),\n",
    "        'eta_ch': np.array([eta]),\n",
    "        'eta_dc': np.array([eta]),\n",
    "        'soc_min': np.array([soc_min]),\n",
    "        'soc_max': np.array([soc_max]),\n",
    "        'names': [f\"{tech}_fleet\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ADAPTIVE TERMINAL SOC TARGET COMPUTATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_adaptive_soc_target(net_load_forecast, theta, soc_min=0.05,\n",
    "                               margin_no_peak=0.07, margin_one_peak=0.03,\n",
    "                               margin_multi_peak=0.02, soc_target_cap=0.15):\n",
    "    \"\"\"\n",
    "    Compute adaptive terminal SOC target based on forecast peak severity.\n",
    "    \n",
    "    Strategy:\n",
    "    - No peaks expected → Higher target (more reserves for uncertainty)\n",
    "    - One peak expected → Moderate target (balanced approach)\n",
    "    - Multiple peaks → Lower target (aggressive discharge justified)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    net_load_forecast : array-like\n",
    "        Net load forecast over MPC horizon (MW)\n",
    "    theta : float\n",
    "        Peak-shaving threshold (MW)\n",
    "    soc_min : float\n",
    "        Hard minimum SOC enforced at each timestep\n",
    "    margin_no_peak : float\n",
    "        Additional margin above soc_min when no peaks detected\n",
    "    margin_one_peak : float\n",
    "        Additional margin when exactly one peak detected\n",
    "    margin_multi_peak : float\n",
    "        Additional margin when multiple peaks detected\n",
    "    soc_target_cap : float\n",
    "        Maximum allowable target to avoid over-conservative behavior\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Computed terminal SOC target (strictly > soc_min)\n",
    "    \"\"\"\n",
    "    nl = np.asarray(net_load_forecast).ravel()\n",
    "    \n",
    "    # Count how many timesteps exceed threshold\n",
    "    over = np.maximum(0.0, nl - theta - 1e-6)\n",
    "    num_peaks = int(np.count_nonzero(over > 0))\n",
    "    \n",
    "    # Select margin based on peak count\n",
    "    if num_peaks == 0:\n",
    "        margin = margin_no_peak\n",
    "    elif num_peaks == 1:\n",
    "        margin = margin_one_peak\n",
    "    else:\n",
    "        margin = margin_multi_peak\n",
    "    \n",
    "    # Compute target with bounds\n",
    "    x_target = np.clip(soc_min + margin, soc_min + 1e-6, soc_target_cap)\n",
    "    \n",
    "    return float(x_target)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MPC OPTIMIZER CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class MPCOptimizer:\n",
    "    \"\"\"\n",
    "    Model Predictive Control optimizer for battery energy storage systems.\n",
    "    \n",
    "    Solves a receding-horizon optimal control problem to minimize total costs\n",
    "    (energy, demand charges, degradation) while respecting system constraints.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    N : int\n",
    "        Optimization horizon length (timesteps)\n",
    "    deltaT : float\n",
    "        Timestep duration (hours), fixed at 0.25 for 15-min intervals\n",
    "    G : int\n",
    "        Number of battery groups\n",
    "    cap_mwh : np.ndarray\n",
    "        Total capacity per group (MWh)\n",
    "    pmax_mw : np.ndarray\n",
    "        Maximum power per group (MW)\n",
    "    eta_ch : np.ndarray\n",
    "        Charging efficiency per group\n",
    "    eta_dc : np.ndarray\n",
    "        Discharging efficiency per group\n",
    "    soc_min : np.ndarray\n",
    "        Minimum SOC per group\n",
    "    soc_max : np.ndarray\n",
    "        Maximum SOC per group\n",
    "    c_energy : float\n",
    "        Energy price (€/kWh)\n",
    "    c_peak : float\n",
    "        Demand charge (€/kW/year)\n",
    "    use_gurobi : bool\n",
    "        Whether to use Gurobi (True) or CLARABEL (False)\n",
    "    gurobi_opts : dict\n",
    "        Options passed to Gurobi solver\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, battery_params, tariff_params, degradation_params,\n",
    "                 horizon_length=96, use_gurobi=True, gurobi_opts=None):\n",
    "        \"\"\"\n",
    "        Initialize MPC optimizer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        battery_params : dict\n",
    "            Battery configuration (see build_battery_fleet)\n",
    "        tariff_params : dict\n",
    "            Simplified tariff structure with keys:\n",
    "            - 'c_energy': float, energy price (€/kWh)\n",
    "            - 'c_peak': float, demand charge (€/kW/year)\n",
    "        degradation_params : dict\n",
    "            Degradation model parameters:\n",
    "            - 'rho1': float, SOC variance penalty weight\n",
    "            - 'rho2': float, power cycling penalty weight\n",
    "        horizon_length : int\n",
    "            MPC horizon length in timesteps (default=96 for 24 hours)\n",
    "        use_gurobi : bool\n",
    "            Use Gurobi solver if True, else CLARABEL (default=True)\n",
    "        gurobi_opts : dict, optional\n",
    "            Gurobi solver options (e.g., {'TimeLimit': 30, 'Threads': 8})\n",
    "        \"\"\"\n",
    "        self.N = horizon_length\n",
    "        self.deltaT = 0.25\n",
    "        \n",
    "        # Parse battery fleet\n",
    "        fleet = build_battery_fleet(battery_params)\n",
    "        self.G = fleet['n_groups']\n",
    "        self.cap_mwh = fleet['cap_mwh']\n",
    "        self.pmax_mw = fleet['pmax_mw']\n",
    "        self.eta_ch = fleet['eta_ch']\n",
    "        self.eta_dc = fleet['eta_dc']\n",
    "        self.soc_min = fleet['soc_min']\n",
    "        self.soc_max = fleet['soc_max']\n",
    "        self.names = fleet['names']\n",
    "        \n",
    "        # Simplified tariff parameters (single-tier pricing)\n",
    "        self.c_energy = tariff_params['c_energy']\n",
    "        self.c_peak = tariff_params['c_peak']\n",
    "        \n",
    "        # Degradation parameters\n",
    "        self.rho1 = degradation_params['rho1']\n",
    "        self.rho2 = degradation_params['rho2']\n",
    "        \n",
    "        # Solver configuration\n",
    "        self.use_gurobi = use_gurobi\n",
    "        self.gurobi_opts = gurobi_opts or {}\n",
    "    \n",
    "    def optimize_timestep(self, net_load_forecast, current_socs,\n",
    "                         cumulative_energy, current_max_peak):\n",
    "        \"\"\"\n",
    "        Solve MPC optimization for one timestep.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        net_load_forecast : array-like\n",
    "            Net load forecast (MW), shape (N,)\n",
    "        current_socs : array-like\n",
    "            Current SOC per group, shape (G,)\n",
    "        cumulative_energy : float\n",
    "            Cumulative energy (kWh) - unused in simplified version\n",
    "        current_max_peak : float\n",
    "            Current peak (kW) - unused in simplified version\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Optimization results with keys:\n",
    "            - 'status': str, solver status\n",
    "            - 'P_grid_from': float, grid import (MW)\n",
    "            - 'P_dump': float, curtailed solar (MW)\n",
    "            - 'next_socs': list, updated SOC per group\n",
    "            - 'P_chg': float, total charging power (MW)\n",
    "            - 'P_dchg': float, total discharging power (MW)\n",
    "            - 'energy_cost': float, energy cost (€)\n",
    "            - 'peak_cost': float, demand charge (€)\n",
    "            - 'degradation_cost': float, degradation penalty (€)\n",
    "        \"\"\"\n",
    "        current_socs = np.asarray(current_socs, dtype=float)\n",
    "        \n",
    "        # Adaptive terminal SOC target\n",
    "        x_target = compute_adaptive_soc_target(\n",
    "            net_load_forecast, theta=1.6, soc_min=float(np.min(self.soc_min))\n",
    "        )\n",
    "        \n",
    "        # Decision variables\n",
    "        x = cp.Variable((self.G, self.N + 1))       # SOC trajectories\n",
    "        P_chg = cp.Variable((self.G, self.N))       # Charging power\n",
    "        P_dchg = cp.Variable((self.G, self.N))      # Discharging power\n",
    "        P_grid = cp.Variable(self.N)                # Grid import\n",
    "        P_dump = cp.Variable(self.N)                # Curtailed solar\n",
    "        theta = cp.Parameter(nonneg=True, value=1.6)  # Peak threshold\n",
    "        \n",
    "        # Constraints\n",
    "        constraints = [x[:, 0] == current_socs]\n",
    "        \n",
    "        for k in range(self.N):\n",
    "            # SOC dynamics: x[k+1] = x[k] - (Δt/E) * (P_dchg/η_dc - P_chg*η_ch)\n",
    "            vec_gain = self.deltaT / self.cap_mwh\n",
    "            term = P_dchg[:, k] / self.eta_dc - cp.multiply(P_chg[:, k], self.eta_ch)\n",
    "            \n",
    "            constraints += [\n",
    "                x[:, k+1] == x[:, k] - cp.multiply(vec_gain, term),\n",
    "                x[:, k+1] >= self.soc_min,\n",
    "                x[:, k+1] <= self.soc_max,\n",
    "                P_chg[:, k] >= 0,\n",
    "                P_chg[:, k] <= self.pmax_mw,\n",
    "                P_dchg[:, k] >= 0,\n",
    "                P_dchg[:, k] <= self.pmax_mw,\n",
    "            ]\n",
    "            \n",
    "            # Charging from excess solar only\n",
    "            constraints += [\n",
    "                cp.sum(P_chg[:, k]) <= cp.maximum(0, -net_load_forecast[k])\n",
    "            ]\n",
    "            \n",
    "            # Curtailment logic\n",
    "            constraints += [\n",
    "                P_dump[k] >= 0,\n",
    "                P_dump[k] >= cp.maximum(0, -net_load_forecast[k]) - cp.sum(P_chg[:, k]),\n",
    "                P_dump[k] <= cp.maximum(0, -net_load_forecast[k])\n",
    "            ]\n",
    "            \n",
    "            # Grid import cap\n",
    "            constraints += [\n",
    "                P_grid[k] >= 0,\n",
    "                P_grid[k] <= theta\n",
    "            ]\n",
    "            \n",
    "            # Power balance\n",
    "            constraints += [\n",
    "                net_load_forecast[k] == P_grid[k] + cp.sum(P_dchg[:, k]) \n",
    "                                       - cp.sum(P_chg[:, k]) + P_dump[k]\n",
    "            ]\n",
    "        \n",
    "        # Objective components\n",
    "        energy_cost = (self.c_energy * 1000) * cp.sum(P_grid) * self.deltaT\n",
    "        peak_cost = (self.c_peak * 1000) * theta\n",
    "        soc_penalty = self.rho1 * cp.sum_squares(x)\n",
    "        power_penalty = self.rho2 * (cp.sum_squares(P_chg) + cp.sum_squares(P_dchg))\n",
    "        degradation_cost = soc_penalty + power_penalty\n",
    "        dump_penalty = (self.c_energy * 1000) * cp.sum(P_dump) * self.deltaT\n",
    "        \n",
    "        obj = cp.Minimize(energy_cost + peak_cost + degradation_cost + dump_penalty)\n",
    "        \n",
    "        # Solve\n",
    "        prob = cp.Problem(obj, constraints)\n",
    "        status = \"unknown\"\n",
    "        \n",
    "        try:\n",
    "            if self.use_gurobi:\n",
    "                prob.solve(\n",
    "                    solver=cp.GUROBI,\n",
    "                    qcp=True,  # Enable quadratic constraint programming\n",
    "                    reoptimize=True,\n",
    "                    verbose=self.gurobi_opts.get(\"OutputFlag\", 0) == 1,\n",
    "                    **self.gurobi_opts\n",
    "                )\n",
    "                status = prob.status\n",
    "            else:\n",
    "                prob.solve(solver=cp.CLARABEL, verbose=False)\n",
    "                status = prob.status\n",
    "        except Exception as e:\n",
    "            status = f\"solver_error: {str(e)}\"\n",
    "            print(f\"Solver error: {e}\")\n",
    "        \n",
    "        # Extract results\n",
    "        res = {'status': status}\n",
    "        \n",
    "        if status in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "            res.update({\n",
    "                'P_grid_from': float(P_grid.value[0]),\n",
    "                'P_dump': float(P_dump.value[0]),\n",
    "                'next_socs': x.value[:, 1].tolist(),\n",
    "                'P_chg': float(np.sum(P_chg.value[:, 0])),\n",
    "                'P_dchg': float(np.sum(P_dchg.value[:, 0])),\n",
    "                'energy_cost': float(energy_cost.value),\n",
    "                'peak_cost': float(peak_cost.value),\n",
    "                'degradation_cost': float(degradation_cost.value),\n",
    "                'soc_penalty': float(soc_penalty.value),\n",
    "                'power_penalty': float(power_penalty.value),\n",
    "                'dump_penalty': float(dump_penalty.value),\n",
    "            })\n",
    "        else:\n",
    "            res.update({\n",
    "                'P_grid_from': 0.0, 'P_dump': 0.0,\n",
    "                'next_socs': current_socs.tolist(),\n",
    "                'P_chg': 0.0, 'P_dchg': 0.0,\n",
    "                'energy_cost': 0.0, 'peak_cost': 0.0,\n",
    "                'degradation_cost': 0.0, 'soc_penalty': 0.0,\n",
    "                'power_penalty': 0.0, 'dump_penalty': 0.0,\n",
    "            })\n",
    "        \n",
    "        return res\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ANNUAL SIMULATION\n",
    "# =============================================================================\n",
    "\n",
    "def run_year_simulation(df, PVSF=2, battery_params=None, tariff_params=None,\n",
    "                       degradation_params=None, use_gurobi=True, gurobi_opts=None):\n",
    "    \"\"\"\n",
    "    Run full-year MPC simulation with receding horizon control.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input data with columns:\n",
    "        - 'PV_generation_kW': PV generation (kW), 15-min resolution\n",
    "        - 'load_demand_kW': Load demand (kW), 15-min resolution\n",
    "    PVSF : float\n",
    "        PV scaling factor (default=2)\n",
    "    battery_params : dict, optional\n",
    "        Battery configuration\n",
    "    tariff_params : dict, optional\n",
    "        Simplified tariff structure with 'c_energy' and 'c_peak'\n",
    "    degradation_params : dict, optional\n",
    "        Degradation model parameters\n",
    "    use_gurobi : bool\n",
    "        Use Gurobi solver if True, else CLARABEL (default=True)\n",
    "    gurobi_opts : dict, optional\n",
    "        Gurobi solver options\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Simulation results with time-series data\n",
    "    cumulative_energy : float\n",
    "        Total annual energy (kWh)\n",
    "    peak_power : float\n",
    "        Annual peak demand (kW)\n",
    "    tariff_params : dict\n",
    "        Applied tariff parameters\n",
    "    degradation_params : dict\n",
    "        Applied degradation parameters\n",
    "    \"\"\"\n",
    "    # Construct net load forecast\n",
    "    solar = (PVSF * df['PV_generation_kW'].values) / 1000.0  # MW\n",
    "    load = df['load_demand_kW'].values / 1000.0  # MW\n",
    "    net_load = load - solar  # MW\n",
    "    \n",
    "    # Simulation setup\n",
    "    N = 96  # Horizon length\n",
    "    days = len(net_load) // N\n",
    "    total_steps = days * N\n",
    "    deltaT = 0.25\n",
    "    \n",
    "    # Default parameters\n",
    "    if battery_params is None:\n",
    "        battery_params = {\n",
    "            'capacity_per_battery': 932 / 1000,  # MWh\n",
    "            'max_power_per_battery': 0.5 * 932 / 1000,  # MW\n",
    "            'efficiency': 0.986**2,\n",
    "            'soc_min': 0.05, 'soc_max': 0.95,\n",
    "            'battery_type': 'LFP',\n",
    "            'n_units': 2\n",
    "        }\n",
    "    \n",
    "    if tariff_params is None:\n",
    "        # Simplified single-tier tariff structure\n",
    "        tariff_params = {\n",
    "            'c_energy': 0.40,   # €/kWh (example average rate)\n",
    "            'c_peak': 80.0      # €/kW/year (example average demand charge)\n",
    "        }\n",
    "    \n",
    "    if degradation_params is None:\n",
    "        degradation_params = {'rho1': 0.25, 'rho2': 5.0}\n",
    "    \n",
    "    if gurobi_opts is None:\n",
    "        gurobi_opts = {\n",
    "            \"TimeLimit\": 30,           # Maximum solve time (seconds)\n",
    "            \"FeasibilityTol\": 1e-8,    # Primal feasibility tolerance\n",
    "            \"OptimalityTol\": 1e-6,     # Dual feasibility tolerance\n",
    "            \"QCPDual\": 0,              # Disable dual variables for QCP\n",
    "            \"Threads\": 0,              # Use all available cores (0 = auto)\n",
    "            \"OutputFlag\": 0            # Suppress Gurobi console output\n",
    "        }\n",
    "    \n",
    "    # Initialize MPC\n",
    "    mpc = MPCOptimizer(\n",
    "        battery_params, tariff_params, degradation_params,\n",
    "        horizon_length=N, use_gurobi=use_gurobi, gurobi_opts=gurobi_opts\n",
    "    )\n",
    "    \n",
    "    # Allocate results\n",
    "    results = {\n",
    "        'grid_import': np.zeros(total_steps),\n",
    "        'energy_dumped': np.zeros(total_steps),\n",
    "        'battery_soc': np.zeros(total_steps + 1),\n",
    "        'battery_chg': np.zeros(total_steps),\n",
    "        'battery_dchg': np.zeros(total_steps),\n",
    "        'energy_cost': np.zeros(total_steps),\n",
    "        'peak_cost': np.zeros(total_steps),\n",
    "        'degradation_cost': np.zeros(total_steps),\n",
    "    }\n",
    "    \n",
    "    # Initialize state\n",
    "    current_socs = mpc.soc_min.copy()\n",
    "    cumulative_energy = 0.0\n",
    "    current_max_peak = 0.0\n",
    "    \n",
    "    # Simulation loop\n",
    "    for step in range(total_steps):\n",
    "        # Build forecast\n",
    "        forecast_start = step\n",
    "        forecast_end = min(step + N, total_steps)\n",
    "        \n",
    "        if forecast_end < step + N:\n",
    "            forecast = np.zeros(N)\n",
    "            avail = forecast_end - forecast_start\n",
    "            forecast[:avail] = net_load[forecast_start:forecast_end]\n",
    "        else:\n",
    "            forecast = net_load[forecast_start:forecast_end]\n",
    "        \n",
    "        # Optimize\n",
    "        step_res = mpc.optimize_timestep(forecast, current_socs,\n",
    "                                        cumulative_energy, current_max_peak)\n",
    "        \n",
    "        # Store results\n",
    "        results['grid_import'][step] = step_res['P_grid_from']\n",
    "        results['energy_dumped'][step] = step_res['P_dump']\n",
    "        results['battery_soc'][step] = current_socs[0]\n",
    "        results['battery_chg'][step] = step_res['P_chg']\n",
    "        results['battery_dchg'][step] = step_res['P_dchg']\n",
    "        results['energy_cost'][step] = step_res['energy_cost']\n",
    "        results['peak_cost'][step] = step_res['peak_cost']\n",
    "        results['degradation_cost'][step] = step_res['degradation_cost']\n",
    "        \n",
    "        # Update state\n",
    "        current_socs = np.array(step_res['next_socs'])\n",
    "        cumulative_energy += results['grid_import'][step] * deltaT * 1000.0\n",
    "        current_max_peak = max(current_max_peak, results['grid_import'][step] * 1000.0)\n",
    "        \n",
    "        # Progress reporting\n",
    "        if (step + 1) % (24 * 4) == 0:\n",
    "            day = (step + 1) // (24 * 4)\n",
    "            print(f\"Day {day}: Peak={current_max_peak/1000:.3f}MW, \"\n",
    "                  f\"SOC={current_socs[0]:.3f}, Status={step_res['status']}\")\n",
    "    \n",
    "    results['battery_soc'][-1] = current_socs[0]\n",
    "    \n",
    "    return results, cumulative_energy, current_max_peak, tariff_params, degradation_params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_results(results, cumulative_energy, annual_peak_power,\n",
    "                   tariff_params, degradation_params):\n",
    "    \"\"\"\n",
    "    Print annual cost summary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        Simulation results\n",
    "    cumulative_energy : float\n",
    "        Annual energy (kWh)\n",
    "    annual_peak_power : float\n",
    "        Annual peak (kW)\n",
    "    tariff_params : dict\n",
    "        Tariff parameters\n",
    "    degradation_params : dict\n",
    "        Degradation parameters\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Cost summary\n",
    "    \"\"\"\n",
    "    energy_cost = cumulative_energy * tariff_params['c_energy']\n",
    "    peak_cost = annual_peak_power * tariff_params['c_peak']\n",
    "    total_degradation = np.sum(results['degradation_cost'])\n",
    "    total_cost = energy_cost + peak_cost + total_degradation\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANNUAL RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total Energy: {cumulative_energy:,.0f} kWh\")\n",
    "    print(f\"Peak Power: {annual_peak_power:,.1f} kW\")\n",
    "    print(f\"Energy Cost: {energy_cost:,.0f} €\")\n",
    "    print(f\"Peak Cost: {peak_cost:,.0f} €\")\n",
    "    print(f\"Degradation Cost: {total_degradation:,.0f} €\")\n",
    "    print(f\"TOTAL COST: {total_cost:,.0f} €\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'total_energy_kWh': cumulative_energy,\n",
    "        'peak_power_kW': annual_peak_power,\n",
    "        'energy_cost': energy_cost,\n",
    "        'peak_cost': peak_cost,\n",
    "        'degradation_cost': total_degradation,\n",
    "        'total_cost': total_cost\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Usage require user-provided PV generation data.\n",
    "    \n",
    "    Required input CSV with column:\n",
    "    - 'PV_generation_kW': Annual PV generation (kW, 15-min intervals)\n",
    "    \n",
    "    Load demand profile is generated synthetically to match the PV profile length.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load PV generation data (user must provide this file)\n",
    "    df_pv = solardf['PV']\n",
    "    \n",
    "    # Verify required column exists\n",
    "    if 'PV_generation_kW' not in df_pv.columns:\n",
    "        raise ValueError(\"Input file must contain 'PV_generation_kW' column\")\n",
    "    \n",
    "    # Generate synthetic load demand profile\n",
    "    # Characteristics: realistic industrial/commercial load pattern\n",
    "    n_steps = len(df_pv)\n",
    "    n_days = n_steps // 96  # 96 timesteps per day (15-min intervals)\n",
    "    \n",
    "    print(f\"Generating synthetic load profile for {n_days} days ({n_steps} timesteps)...\")\n",
    "    \n",
    "    # Create time index for pattern generation\n",
    "    timestep_of_day = np.tile(np.arange(96), n_days)[:n_steps]  # 0-95 for each day\n",
    "    day_of_year = np.repeat(np.arange(n_days), 96)[:n_steps]\n",
    "    \n",
    "    # Base load profile: typical daily pattern (kW)\n",
    "    # Higher during business hours (08:00-18:00), lower at night\n",
    "    hour_of_day = timestep_of_day / 4.0  # Convert to hours (0-24)\n",
    "    base_daily_pattern = (\n",
    "        500 +  # Minimum base load\n",
    "        800 * np.clip(np.sin((hour_of_day - 6) * np.pi / 12), 0, 1) +  # Daytime peak\n",
    "        200 * np.random.rand(n_steps)  # Random variation\n",
    "    )\n",
    "    \n",
    "    # Seasonal variation: higher load in summer (cooling) and winter (heating)\n",
    "    seasonal_factor = 1.0 + 0.3 * np.cos(2 * np.pi * day_of_year / 365 - np.pi)\n",
    "    \n",
    "    # Weekly pattern: lower load on weekends\n",
    "    day_of_week = (day_of_year % 7)\n",
    "    weekend_factor = np.where((day_of_week == 5) | (day_of_week == 6), 0.7, 1.0)\n",
    "    weekend_factor = np.repeat(weekend_factor, 96)[:n_steps]\n",
    "    \n",
    "    # Random noise and occasional load spikes\n",
    "    random_noise = np.random.normal(1.0, 0.1, n_steps)\n",
    "    random_spikes = np.random.choice([1.0, 1.3], size=n_steps, p=[0.98, 0.02])\n",
    "    \n",
    "    # Combine all components\n",
    "    load_demand_kW = (\n",
    "        base_daily_pattern * \n",
    "        seasonal_factor * \n",
    "        weekend_factor * \n",
    "        random_noise * \n",
    "        random_spikes\n",
    "    )\n",
    "    \n",
    "    # Ensure non-negative and clip to realistic range\n",
    "    load_demand_kW = np.clip(load_demand_kW, 100, 3000)\n",
    "    \n",
    "    # Create final dataframe with both PV generation and synthetic load\n",
    "    df = pd.DataFrame({\n",
    "        'PV_generation_kW': df_pv['PV_generation_kW'].values,\n",
    "        'load_demand_kW': load_demand_kW\n",
    "    })\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nSynthetic Load Profile Statistics:\")\n",
    "    print(f\"  Mean load: {df['load_demand_kW'].mean():.1f} kW\")\n",
    "    print(f\"  Peak load: {df['load_demand_kW'].max():.1f} kW\")\n",
    "    print(f\"  Min load: {df['load_demand_kW'].min():.1f} kW\")\n",
    "    print(f\"  Total energy: {df['load_demand_kW'].sum() * 0.25 / 1000:.1f} MWh\")\n",
    "    print(f\"\\nPV Generation Statistics:\")\n",
    "    print(f\"  Mean generation: {df['PV_generation_kW'].mean():.1f} kW\")\n",
    "    print(f\"  Peak generation: {df['PV_generation_kW'].max():.1f} kW\")\n",
    "    print(f\"  Total energy: {df['PV_generation_kW'].sum() * 0.25 / 1000:.1f} MWh\")\n",
    "    \n",
    "    # Configure system\n",
    "    battery_params = {\n",
    "        'capacity_per_battery': 0.932,  # MWh\n",
    "        'max_power_per_battery': 0.466,  # MW\n",
    "        'efficiency': 0.972,\n",
    "        'soc_min': 0.05, 'soc_max': 0.95,\n",
    "        'battery_type': 'LFP',\n",
    "        'n_units': 2\n",
    "    }\n",
    "    \n",
    "    # Simplified tariff (single-tier)\n",
    "    tariff_params = {\n",
    "        'c_energy': 0.40,  # €/kWh\n",
    "        'c_peak': 80.0     # €/kW/year\n",
    "    }\n",
    "    \n",
    "    # Gurobi solver options (no license credentials in code)\n",
    "    gurobi_opts = {\n",
    "        \"TimeLimit\": 30,\n",
    "        \"Threads\": 8,\n",
    "        \"OutputFlag\": 0\n",
    "    }\n",
    "    \n",
    "    # Run simulation with Gurobi\n",
    "    print(\"\\nStarting MPC simulation...\")\n",
    "    results, cum_e, peak, tariff, degr = run_year_simulation(\n",
    "        df, PVSF=2, \n",
    "        battery_params=battery_params,\n",
    "        tariff_params=tariff_params,\n",
    "        use_gurobi=True,\n",
    "        gurobi_opts=gurobi_opts\n",
    "    )\n",
    "    \n",
    "    # Analyze\n",
    "    summary = analyze_results(results, cum_e, peak, tariff, degr)\n",
    "    \n",
    "    print(\"\\nSimulation complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceafe5b",
   "metadata": {},
   "source": [
    "# 3. LONG-TERM RENEWABLE ENERGY SCENARIO GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860eaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Climate-Driven PV Generation Scenario Analysis\n",
    "\n",
    "This module generates probabilistic PV generation scenarios from climate projection\n",
    "data (SSP pathways) using kernel density estimation and quantile sampling methods.\n",
    "It performs technology sensitivity analysis to separate climate uncertainty from\n",
    "technological improvements in PV panel efficiency.\n",
    "\n",
    "Features:\n",
    "- SSP climate pathway processing (GHI, temperature)\n",
    "- Solar position and POA irradiance calculation (pvlib)\n",
    "- KDE-based synthetic year generation\n",
    "- Quantile-based scenario selection\n",
    "- PV power modeling with temperature effects\n",
    "- Technology sensitivity analysis (panel efficiency variations)\n",
    "\n",
    "Author: Malik Muhammad Abdullah @ Karlsruhe Institute of Technology(Research Facility 2.0 Project)\n",
    "License: European Union Public Licence (EUPL) v1.2 or later\n",
    "Copyright: © 2025 Research Facility 2.0 Consortium\n",
    "\"\"\"\n",
    "\n",
    "# Copyright © 2025 Research Facility 2.0 Consortium\n",
    "#\n",
    "# Licensed under the EUPL, Version 1.2 or – as soon they will be approved by\n",
    "# the European Commission - subsequent versions of the EUPL (the \"Licence\");\n",
    "# You may not use this work except in compliance with the Licence.\n",
    "# You may obtain a copy of the Licence at:\n",
    "#\n",
    "# https://joinup.ec.europa.eu/collection/eupl/eupl-text-eupl-12\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the Licence is distributed on an \"AS IS\" basis,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the Licence for the specific language governing permissions and\n",
    "# limitations under the Licence.\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "from pvlib import solarposition, irradiance\n",
    "from pvlib.location import Location\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Output directory for results\n",
    "OUTPUT_DIR = Path('./scenario_results')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Site parameters (example location - user should modify)\n",
    "SITE_CONFIG = {\n",
    "    'latitude': 49.0,      # Decimal degrees North\n",
    "    'longitude': 8.4,      # Decimal degrees East\n",
    "    'altitude': 110,       # Meters above sea level\n",
    "    'timezone': 'Europe/Berlin',\n",
    "    'name': 'Site_Location'\n",
    "}\n",
    "\n",
    "# PV system parameters (example configuration)\n",
    "PV_CONFIG = {\n",
    "    'N_PV': 800,           # Number of PV panels\n",
    "    'tilt': 5,             # Panel tilt angle (degrees)\n",
    "    'azimuth': 180,        # Panel azimuth (180 = south-facing)\n",
    "    'module_specs': {\n",
    "        'ratedpowerkw': 0.4,           # Rated power per panel (kW)\n",
    "        'SurfaceAream2': 1.92,         # Panel surface area (m²)\n",
    "        'panel_efficiency': 0.20,       # Panel efficiency (20%)\n",
    "        'temp_coeff': -0.0039,         # Temperature coefficient (/°C)\n",
    "        'ref_temp': 25,                # Reference temperature (°C)\n",
    "        'ncot': 45,                    # Nominal cell operating temp (°C)\n",
    "        'Degrading_Factor': 0.88       # System degradation factor\n",
    "    }\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# PV POWER CALCULATION MODEL\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pv_power_flexible(\n",
    "    temperature: np.ndarray,\n",
    "    gti_irradiance: np.ndarray,\n",
    "    N_PV: float,\n",
    "    module_specs: dict = None,\n",
    "    use_rated_power: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate PV power output with temperature corrections.\n",
    "    \n",
    "    Models the relationship between incident irradiance, cell temperature,\n",
    "    and electrical power output, accounting for temperature-dependent\n",
    "    efficiency losses.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    temperature : np.ndarray\n",
    "        Ambient temperature (°C)\n",
    "    gti_irradiance : np.ndarray\n",
    "        Global tilted irradiance on panel surface (W/m²)\n",
    "    N_PV : float\n",
    "        Number of PV panels\n",
    "    module_specs : dict, optional\n",
    "        Module specifications (see PV_CONFIG)\n",
    "    use_rated_power : bool, optional\n",
    "        If True, use rated power method; else use area-efficiency method\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        PV power output (kW)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Cell temperature model: T_cell = T_amb + GTI * (NCOT - 20) / 800\n",
    "    Temperature loss factor: 1 + temp_coeff * (T_cell - T_ref)\n",
    "    \"\"\"\n",
    "    if module_specs is None:\n",
    "        module_specs = PV_CONFIG['module_specs']\n",
    "    \n",
    "    temp_ambient = np.array(temperature)\n",
    "    gti = np.array(gti_irradiance)\n",
    "    \n",
    "    # Calculate cell temperature from ambient and irradiance\n",
    "    cell_temp = temp_ambient + (gti * (module_specs['ncot'] - 20) / 800)\n",
    "    \n",
    "    # Temperature-dependent efficiency correction\n",
    "    temp_loss_factor = 1 + module_specs['temp_coeff'] * (cell_temp - module_specs['ref_temp'])\n",
    "    \n",
    "    if use_rated_power:\n",
    "        # Method 1: Based on rated power (STC conditions)\n",
    "        power_kw = (\n",
    "            (gti / 1000.0) * N_PV * module_specs['ratedpowerkw'] *\n",
    "            temp_loss_factor * module_specs['Degrading_Factor']\n",
    "        )\n",
    "    else:\n",
    "        # Method 2: Based on panel area and efficiency\n",
    "        power_kw = (\n",
    "            gti * N_PV * module_specs['SurfaceAream2'] *\n",
    "            module_specs['panel_efficiency'] * temp_loss_factor *\n",
    "            module_specs['Degrading_Factor'] / 1000.0\n",
    "        )\n",
    "    \n",
    "    return power_kw\n",
    "\n",
    "# =============================================================================\n",
    "# SSP CLIMATE DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def load_ssp_climate_data(base_path):\n",
    "    \"\"\"\n",
    "    Load and standardize SSP climate projection data from CSV files.\n",
    "    \n",
    "    Expected file naming: Files containing 'SSP119', 'SSP126', 'SSP245',\n",
    "    'SSP370', 'SSP460', or 'SSP585' in filename.\n",
    "    \n",
    "    Expected columns:\n",
    "    - Datetime column (first column)\n",
    "    - Column containing 'rsds' (surface downwelling shortwave radiation, W/m²)\n",
    "    - Column containing 'tas' (near-surface air temperature, K)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base_path : str or Path\n",
    "        Directory containing SSP climate data CSV files\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Combined climate data with columns:\n",
    "        - datetime: timestamp\n",
    "        - year, month, hour, day_of_year: temporal indices\n",
    "        - ssp: scenario identifier (e.g., 'ssp126')\n",
    "        - ghi: global horizontal irradiance (W/m²)\n",
    "        - temperature_C: air temperature (°C)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    SSP pathways represent different climate futures:\n",
    "    - SSP1-1.9, SSP1-2.6: Low emissions, Paris Agreement targets\n",
    "    - SSP2-4.5: Middle-of-the-road, moderate emissions\n",
    "    - SSP3-7.0, SSP5-8.5: High emissions, business-as-usual\n",
    "    \n",
    "    Data typically sourced from CMIP6 climate models via CORDEX or similar.\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    ssp_files = sorted(list(base_path.glob('*SSP*.csv')))\n",
    "    \n",
    "    if len(ssp_files) == 0:\n",
    "        raise FileNotFoundError(f\"No SSP files found in {base_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"STEP 1: LOADING SSP CLIMATE DATA\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Found {len(ssp_files)} SSP files\\n\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in ssp_files:\n",
    "        filename = file_path.stem\n",
    "        \n",
    "        # Identify SSP scenario from filename\n",
    "        ssp = None\n",
    "        for ssp_id in ['119', '126', '245', '370', '460', '585']:\n",
    "            if f'SSP{ssp_id}' in filename or f'ssp{ssp_id}' in filename.lower():\n",
    "                ssp = f'ssp{ssp_id}'\n",
    "                break\n",
    "        \n",
    "        if not ssp:\n",
    "            print(f\"Skipping {filename}: No recognized SSP identifier\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Loading {ssp}...\")\n",
    "        \n",
    "        try:\n",
    "            df_temp = pd.read_csv(file_path)\n",
    "            cols = df_temp.columns.tolist()\n",
    "            \n",
    "            # First column assumed to be datetime\n",
    "            datetime_col = cols[0]\n",
    "            \n",
    "            # Find irradiance column (rsds = surface downwelling shortwave)\n",
    "            rsds_col = None\n",
    "            for col in cols:\n",
    "                if 'rsds' in col.lower() and 'rsdscs' not in col.lower():\n",
    "                    rsds_col = col\n",
    "                    break\n",
    "            \n",
    "            # Find temperature column (tas = near-surface air temperature)\n",
    "            tas_col = None\n",
    "            for col in cols:\n",
    "                if 'tas' in col.lower():\n",
    "                    tas_col = col\n",
    "                    break\n",
    "            \n",
    "            if not rsds_col or not tas_col:\n",
    "                print(f\"  ERROR: Missing required columns (rsds, tas)\")\n",
    "                continue\n",
    "            \n",
    "            # Standardize columns\n",
    "            df_temp['datetime'] = pd.to_datetime(df_temp[datetime_col])\n",
    "            df_temp['ssp'] = ssp\n",
    "            df_temp['ghi'] = df_temp[rsds_col]  # W/m²\n",
    "            df_temp['temperature_K'] = df_temp[tas_col]  # Kelvin\n",
    "            df_temp['temperature_C'] = df_temp['temperature_K'] - 273.15  # Convert to Celsius\n",
    "            df_temp['year'] = df_temp['datetime'].dt.year\n",
    "            df_temp['month'] = df_temp['datetime'].dt.month\n",
    "            df_temp['hour'] = df_temp['datetime'].dt.hour\n",
    "            df_temp['day_of_year'] = df_temp['datetime'].dt.dayofyear\n",
    "            \n",
    "            # Select final columns\n",
    "            final_cols = ['datetime', 'year', 'month', 'hour', 'day_of_year',\n",
    "                         'ssp', 'ghi', 'temperature_C']\n",
    "            df_clean = df_temp[final_cols].copy()\n",
    "            \n",
    "            print(f\"  Loaded {len(df_clean):,} records\")\n",
    "            print(f\"  Years: {df_clean['year'].min()}-{df_clean['year'].max()}\")\n",
    "            \n",
    "            all_data.append(df_clean)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        raise ValueError(\"No valid SSP files were loaded\")\n",
    "    \n",
    "    combined = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Total records loaded: {len(combined):,} across {len(all_data)} SSPs\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    output_file = OUTPUT_DIR / 'step1_raw_climate_data.csv'\n",
    "    combined.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Saved: {output_file}\\n\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "# =============================================================================\n",
    "# IRRADIANCE CONVERSION TO TILTED SURFACE\n",
    "# =============================================================================\n",
    "\n",
    "def convert_to_tilted_irradiance_pvlib(df, site_config=SITE_CONFIG, pv_config=PV_CONFIG):\n",
    "    \"\"\"\n",
    "    Convert GHI to plane-of-array (POA) irradiance using pvlib.\n",
    "    \n",
    "    Process:\n",
    "    1. Calculate solar position (zenith, azimuth) for site and times\n",
    "    2. Decompose GHI into DNI (direct) and DHI (diffuse) components\n",
    "    3. Apply transposition model to calculate POA irradiance\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Climate data with 'datetime', 'ghi', 'temperature_C'\n",
    "    site_config : dict\n",
    "        Site location parameters\n",
    "    pv_config : dict\n",
    "        PV system parameters (tilt, azimuth)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Input data augmented with:\n",
    "        - zenith, azimuth_sun: solar position\n",
    "        - dni, dhi: direct and diffuse horizontal irradiance\n",
    "        - poa_global, poa_direct, poa_diffuse: plane-of-array components\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Uses Erbs decomposition model (1982) for GHI → DNI/DHI separation.\n",
    "    Uses isotropic sky model for transposition to tilted surface.\n",
    "    \"\"\"\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"STEP 2: CONVERTING TO TILTED IRRADIANCE (PVLIB)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Site: {site_config['name']}\")\n",
    "    print(f\"Location: ({site_config['latitude']:.4f}°N, {site_config['longitude']:.4f}°E)\")\n",
    "    print(f\"Tilt: {pv_config['tilt']}°, Azimuth: {pv_config['azimuth']}°\\n\")\n",
    "    \n",
    "    location = Location(\n",
    "        latitude=site_config['latitude'],\n",
    "        longitude=site_config['longitude'],\n",
    "        altitude=site_config['altitude'],\n",
    "        tz=site_config['timezone']\n",
    "    )\n",
    "    \n",
    "    df_work = df.copy()\n",
    "    \n",
    "    print(\"Calculating solar position...\")\n",
    "    solar_pos = location.get_solarposition(df_work['datetime'])\n",
    "    \n",
    "    zenith = solar_pos['apparent_zenith'].values\n",
    "    azimuth = solar_pos['azimuth'].values\n",
    "    \n",
    "    df_work['zenith'] = zenith\n",
    "    df_work['azimuth_sun'] = azimuth\n",
    "    \n",
    "    print(\"Decomposing GHI into DNI and DHI (Erbs model)...\")\n",
    "    \n",
    "    # Calculate clearness index (kt)\n",
    "    zenith_rad = np.radians(zenith)\n",
    "    cos_zenith = np.cos(zenith_rad)\n",
    "    cos_zenith = np.clip(cos_zenith, 0, 1)\n",
    "    \n",
    "    daytime_mask = zenith < 90\n",
    "    \n",
    "    # Extraterrestrial irradiance = 1367 W/m²\n",
    "    df_work['kt'] = 0.0\n",
    "    df_work.loc[daytime_mask, 'kt'] = np.clip(\n",
    "        df_work.loc[daytime_mask, 'ghi'].values / (1367 * cos_zenith[daytime_mask] + 1e-10),\n",
    "        0, 1\n",
    "    )\n",
    "    \n",
    "    # Erbs diffuse fraction model\n",
    "    kt = df_work['kt'].values\n",
    "    df_work['diffuse_fraction'] = np.where(\n",
    "        kt <= 0.22,\n",
    "        1 - 0.09 * kt,\n",
    "        np.where(\n",
    "            kt <= 0.8,\n",
    "            0.9511 - 0.1604 * kt + 4.388 * kt**2 - 16.638 * kt**3 + 12.336 * kt**4,\n",
    "            0.165\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Calculate DHI and DNI\n",
    "    df_work['dhi'] = df_work['ghi'] * df_work['diffuse_fraction']\n",
    "    \n",
    "    df_work['dni'] = 0.0\n",
    "    df_work.loc[daytime_mask, 'dni'] = np.clip(\n",
    "        (df_work.loc[daytime_mask, 'ghi'].values - df_work.loc[daytime_mask, 'dhi'].values) / \n",
    "        (cos_zenith[daytime_mask] + 1e-10),\n",
    "        0, None\n",
    "    )\n",
    "    \n",
    "    print(\"Calculating POA irradiance...\")\n",
    "    \n",
    "    # Transposition to tilted surface\n",
    "    poa_irradiance = irradiance.get_total_irradiance(\n",
    "        surface_tilt=pv_config['tilt'],\n",
    "        surface_azimuth=pv_config['azimuth'],\n",
    "        solar_zenith=zenith,\n",
    "        solar_azimuth=azimuth,\n",
    "        dni=df_work['dni'].values,\n",
    "        ghi=df_work['ghi'].values,\n",
    "        dhi=df_work['dhi'].values,\n",
    "        model='isotropic'\n",
    "    )\n",
    "    \n",
    "    # Extract POA components (returns dict with arrays)\n",
    "    df_work['poa_global'] = poa_irradiance['poa_global']\n",
    "    df_work['poa_direct'] = poa_irradiance['poa_direct']\n",
    "    df_work['poa_diffuse'] = poa_irradiance['poa_diffuse']\n",
    "    \n",
    "    # Clip to physical limits\n",
    "    df_work['poa_global'] = df_work['poa_global'].clip(lower=0, upper=1500)\n",
    "    df_work['poa_direct'] = df_work['poa_direct'].clip(lower=0, upper=1200)\n",
    "    df_work['poa_diffuse'] = df_work['poa_diffuse'].clip(lower=0, upper=500)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Irradiance conversion complete\")\n",
    "    print(f\"Mean GHI: {df_work['ghi'].mean():.1f} W/m²\")\n",
    "    print(f\"Mean POA: {df_work['poa_global'].mean():.1f} W/m²\")\n",
    "    print(f\"POA/GHI ratio: {(df_work['poa_global'].mean() / (df_work['ghi'].mean() + 1e-10)):.3f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    output_file = OUTPUT_DIR / 'step2_climate_with_poa_irradiance.csv'\n",
    "    cols_to_save = ['datetime', 'year', 'month', 'hour', 'day_of_year', 'ssp',\n",
    "                    'ghi', 'dni', 'dhi', 'poa_global', 'poa_direct', 'poa_diffuse',\n",
    "                    'temperature_C', 'zenith']\n",
    "    df_work[cols_to_save].to_csv(output_file, index=False)\n",
    "    print(f\"✓ Saved: {output_file}\\n\")\n",
    "    \n",
    "    return df_work\n",
    "\n",
    "# =============================================================================\n",
    "# KDE-BASED SYNTHETIC YEAR GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_kde_synthetic_years(ssp_data, n_synthetic_years=100, perturbation_strength=0.2):\n",
    "    \"\"\"\n",
    "    Generate synthetic years using conditional kernel density estimation.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Select a template year (closest to median annual irradiance)\n",
    "    2. For each hour, sample from KDE trained on similar hours across all years\n",
    "    3. Blend template values with KDE samples for smooth variation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ssp_data : pd.DataFrame\n",
    "        Climate data for one SSP scenario\n",
    "    n_synthetic_years : int\n",
    "        Number of synthetic years to generate\n",
    "    perturbation_strength : float\n",
    "        Blend factor: 0=template only, 1=KDE only\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of pd.DataFrame\n",
    "        Synthetic year dataframes\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Conditional KDE preserves temporal correlations by sampling from\n",
    "    similar times of day and year, avoiding physically unrealistic\n",
    "    combinations (e.g., summer temperature with winter irradiance).\n",
    "    \"\"\"\n",
    "    # Select template year (closest to median annual irradiance)\n",
    "    annual_radiation = ssp_data.groupby('year')['poa_global'].mean()\n",
    "    median_radiation = annual_radiation.median()\n",
    "    \n",
    "    closest_idx = (annual_radiation - median_radiation).abs().argmin()\n",
    "    template_year = annual_radiation.index[closest_idx]\n",
    "    \n",
    "    template_data = ssp_data[ssp_data['year'] == template_year].copy().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  Template year: {template_year} ({len(template_data)} hours)\")\n",
    "    \n",
    "    synthetic_years = []\n",
    "    \n",
    "    for synth_idx in range(n_synthetic_years):\n",
    "        synthetic_records = []\n",
    "        \n",
    "        for idx, template_row in template_data.iterrows():\n",
    "            hour = template_row['hour']\n",
    "            month = template_row['month']\n",
    "            \n",
    "            # Find similar hours (±1 hour window, same month)\n",
    "            similar_mask = (\n",
    "                (ssp_data['month'] == month) &\n",
    "                (ssp_data['hour'] >= hour - 1) &\n",
    "                (ssp_data['hour'] <= hour + 1)\n",
    "            )\n",
    "            similar_data = ssp_data[similar_mask][['ghi', 'poa_global', 'temperature_C']].values\n",
    "            \n",
    "            if len(similar_data) >= 10:\n",
    "                try:\n",
    "                    # Train KDE and sample\n",
    "                    kde = gaussian_kde(similar_data.T)\n",
    "                    sampled = kde.resample(1).T[0]\n",
    "                    \n",
    "                    # Blend template and KDE sample\n",
    "                    new_ghi = perturbation_strength * template_row['ghi'] + (1 - perturbation_strength) * max(0, sampled[0])\n",
    "                    new_poa = perturbation_strength * template_row['poa_global'] + (1 - perturbation_strength) * max(0, sampled[1])\n",
    "                    new_temperature = perturbation_strength * template_row['temperature_C'] + (1 - perturbation_strength) * sampled[2]\n",
    "                except:\n",
    "                    # Fallback: random sample from similar data\n",
    "                    rand_idx = np.random.randint(len(similar_data))\n",
    "                    new_ghi = max(0, similar_data[rand_idx, 0])\n",
    "                    new_poa = max(0, similar_data[rand_idx, 1])\n",
    "                    new_temperature = similar_data[rand_idx, 2]\n",
    "            else:\n",
    "                # Not enough data: use template values\n",
    "                new_ghi = template_row['ghi']\n",
    "                new_poa = template_row['poa_global']\n",
    "                new_temperature = template_row['temperature_C']\n",
    "            \n",
    "            synthetic_records.append({\n",
    "                'datetime': template_row['datetime'],\n",
    "                'year': template_year,\n",
    "                'month': month,\n",
    "                'hour': hour,\n",
    "                'day_of_year': template_row['day_of_year'],\n",
    "                'ssp': template_row['ssp'],\n",
    "                'ghi': new_ghi,\n",
    "                'poa_global': new_poa,\n",
    "                'temperature_C': new_temperature,\n",
    "                'synthetic_id': synth_idx\n",
    "            })\n",
    "        \n",
    "        synth_year_df = pd.DataFrame(synthetic_records)\n",
    "        synthetic_years.append(synth_year_df)\n",
    "        \n",
    "        if (synth_idx + 1) % 20 == 0:\n",
    "            print(f\"    Generated {synth_idx + 1}/{n_synthetic_years}...\")\n",
    "    \n",
    "    return synthetic_years\n",
    "\n",
    "def select_quantile_scenarios_from_synthetic(synthetic_years, quantiles=[0.1, 0.5, 0.9]):\n",
    "    \"\"\"\n",
    "    Select representative scenarios using quantile-based sampling.\n",
    "    \n",
    "    Ranks synthetic years by annual mean POA irradiance and selects\n",
    "    scenarios at specified quantiles (e.g., 10th, 50th, 90th percentile).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    synthetic_years : list of pd.DataFrame\n",
    "        Generated synthetic years\n",
    "    quantiles : list of float\n",
    "        Quantiles to select (0-1 scale)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Selected scenarios with quantile labels\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Quantile selection provides:\n",
    "    - Q10: Low-irradiance scenario (pessimistic)\n",
    "    - Q50: Median scenario (expected)\n",
    "    - Q90: High-irradiance scenario (optimistic)\n",
    "    \"\"\"\n",
    "    # Calculate annual statistics for each synthetic year\n",
    "    annual_stats = []\n",
    "    for synth_df in synthetic_years:\n",
    "        annual_stats.append({\n",
    "            'synthetic_id': synth_df['synthetic_id'].iloc[0],\n",
    "            'poa_mean': synth_df['poa_global'].mean(),\n",
    "            'temp_mean': synth_df['temperature_C'].mean()\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(annual_stats)\n",
    "    selected_scenarios = []\n",
    "    \n",
    "    for q in quantiles:\n",
    "        # Find synthetic year closest to this quantile\n",
    "        target_poa = stats_df['poa_mean'].quantile(q)\n",
    "        closest_idx = (stats_df['poa_mean'] - target_poa).abs().idxmin()\n",
    "        selected_id = stats_df.loc[closest_idx, 'synthetic_id']\n",
    "        \n",
    "        selected_year = synthetic_years[int(selected_id)].copy()\n",
    "        selected_year['quantile'] = q\n",
    "        selected_year['quantile_label'] = f\"Q{int(q*100)}\"\n",
    "        selected_scenarios.append(selected_year)\n",
    "        \n",
    "        print(f\"    Q{int(q*100)}: POA_mean={stats_df.loc[closest_idx, 'poa_mean']:.1f} W/m²\")\n",
    "    \n",
    "    return pd.concat(selected_scenarios, ignore_index=True)\n",
    "\n",
    "def generate_climate_scenarios(df, n_synthetic_per_ssp=100, quantiles=[0.1, 0.5, 0.9]):\n",
    "    \"\"\"\n",
    "    Generate climate scenarios for all SSP pathways.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Climate data with POA irradiance\n",
    "    n_synthetic_per_ssp : int\n",
    "        Number of synthetic years per SSP\n",
    "    quantiles : list of float\n",
    "        Quantiles to select for final scenarios\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Selected scenarios for all SSPs and quantiles\n",
    "    \"\"\"\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"STEP 3: KDE-BASED SCENARIO GENERATION\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    ssp_list = sorted(df['ssp'].unique())\n",
    "    all_scenarios = []\n",
    "    \n",
    "    for ssp in ssp_list:\n",
    "        print(f\"Processing {ssp}...\")\n",
    "        ssp_data = df[df['ssp'] == ssp].copy()\n",
    "        \n",
    "        synthetic_years = generate_kde_synthetic_years(\n",
    "            ssp_data, \n",
    "            n_synthetic_years=n_synthetic_per_ssp,\n",
    "            perturbation_strength=0.2\n",
    "        )\n",
    "        \n",
    "        selected_scenarios = select_quantile_scenarios_from_synthetic(\n",
    "            synthetic_years, \n",
    "            quantiles=quantiles\n",
    "        )\n",
    "        \n",
    "        all_scenarios.append(selected_scenarios)\n",
    "        print()\n",
    "    \n",
    "    result_df = pd.concat(all_scenarios, ignore_index=True)\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Scenarios generated: {len(result_df):,} records\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    output_file = OUTPUT_DIR / 'step3_climate_scenarios_kde_quantile.csv'\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Saved: {output_file}\\n\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# =============================================================================\n",
    "# PV POWER CALCULATION FOR SCENARIOS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pv_power_for_scenarios(df, pv_config=PV_CONFIG):\n",
    "    \"\"\"\n",
    "    Calculate PV power output for all scenarios.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Scenarios with POA irradiance and temperature\n",
    "    pv_config : dict\n",
    "        PV system configuration\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Input data with 'pv_power_kw' column added\n",
    "    \"\"\"\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"STEP 4: CALCULATING PV POWER OUTPUT\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    df['pv_power_kw'] = calculate_pv_power_flexible(\n",
    "        temperature=df['temperature_C'].values,\n",
    "        gti_irradiance=df['poa_global'].values,\n",
    "        N_PV=pv_config['N_PV'],\n",
    "        use_rated_power=False\n",
    "    )\n",
    "    \n",
    "    # Print summary by SSP and quantile\n",
    "    for ssp in sorted(df['ssp'].unique()):\n",
    "        print(f\"\\n{ssp.upper()}:\")\n",
    "        for q_label in sorted(df[df['ssp'] == ssp]['quantile_label'].unique()):\n",
    "            scenario_data = df[(df['ssp'] == ssp) & (df['quantile_label'] == q_label)]\n",
    "            annual_energy = scenario_data['pv_power_kw'].sum()\n",
    "            \n",
    "            # Calculate capacity factor\n",
    "            rated_capacity_kw = pv_config['N_PV'] * pv_config['module_specs']['ratedpowerkw']\n",
    "            cf = (annual_energy / (rated_capacity_kw * 8760)) * 100\n",
    "            \n",
    "            print(f\"  {q_label}: {annual_energy:,.0f} kWh/year, CF={cf:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    output_file = OUTPUT_DIR / 'step4_scenarios_with_pv_power.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Saved: {output_file}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# TECHNOLOGY SENSITIVITY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def technology_sensitivity_analysis(df_scenarios, pv_config=PV_CONFIG):\n",
    "    \"\"\"\n",
    "    Analyze impact of PV panel efficiency improvements on power generation.\n",
    "    \n",
    "    Separates climate uncertainty from technology uncertainty by fixing\n",
    "    climate scenarios and varying panel efficiency.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_scenarios : pd.DataFrame\n",
    "        Climate scenarios with POA irradiance\n",
    "    pv_config : dict\n",
    "        PV system configuration\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tech_results_df : pd.DataFrame\n",
    "        Detailed results for all technology-climate combinations\n",
    "    tech_summary : pd.DataFrame\n",
    "        Summary statistics by technology scenario\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Technology scenarios represent:\n",
    "    - Current (2025): 20% efficiency (baseline)\n",
    "    - Conservative (2030): 21.5% (+6.5%)\n",
    "    - Moderate (2035): 23% (+14%)\n",
    "    - Advanced (2040): 25% (+24%)\n",
    "    - Future (2045): 27% (+34%, lab-achieved)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"STEP 5: TECHNOLOGY SENSITIVITY ANALYSIS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Analyzing PV panel efficiency variations\\n\")\n",
    "    \n",
    "    # Define efficiency scenarios\n",
    "    efficiency_scenarios = {\n",
    "        'Current_2025': 0.20,        # 20% efficiency (baseline)\n",
    "        'Conservative_2030': 0.215,   # +7.5% improvement\n",
    "        'Moderate_2035': 0.23,        # +15% improvement\n",
    "        'Advanced_2040': 0.25,        # +25% improvement\n",
    "        'Future_2045': 0.27          # +35% improvement\n",
    "    }\n",
    "    \n",
    "    print(\"Technology Scenarios:\")\n",
    "    print(\"-\" * 70)\n",
    "    for tech_name, efficiency in efficiency_scenarios.items():\n",
    "        print(f\"  {tech_name:20s}: {efficiency*100:.2f}% efficiency\")\n",
    "    print()\n",
    "    \n",
    "    all_tech_results = []\n",
    "    \n",
    "    for ssp in sorted(df_scenarios['ssp'].unique()):\n",
    "        for q_label in sorted(df_scenarios[df_scenarios['ssp'] == ssp]['quantile_label'].unique()):\n",
    "            \n",
    "            # Get climate scenario data\n",
    "            scenario_data = df_scenarios[\n",
    "                (df_scenarios['ssp'] == ssp) & \n",
    "                (df_scenarios['quantile_label'] == q_label)\n",
    "            ].copy()\n",
    "            \n",
    "            climate_scenario_id = f\"{ssp}_{q_label}\"\n",
    "            \n",
    "            # Calculate power for each efficiency level\n",
    "            for tech_name, efficiency in efficiency_scenarios.items():\n",
    "                \n",
    "                # Modify module specs with new efficiency\n",
    "                modified_specs = pv_config['module_specs'].copy()\n",
    "                modified_specs['panel_efficiency'] = efficiency\n",
    "                \n",
    "                power_kw = calculate_pv_power_flexible(\n",
    "                    temperature=scenario_data['temperature_C'].values,\n",
    "                    gti_irradiance=scenario_data['poa_global'].values,\n",
    "                    N_PV=pv_config['N_PV'],\n",
    "                    module_specs=modified_specs,\n",
    "                    use_rated_power=False\n",
    "                )\n",
    "                \n",
    "                annual_energy = power_kw.sum()\n",
    "                rated_capacity = pv_config['N_PV'] * pv_config['module_specs']['ratedpowerkw']\n",
    "                capacity_factor = (annual_energy / (rated_capacity * 8760)) * 100\n",
    "                \n",
    "                # Calculate improvement vs baseline\n",
    "                baseline_efficiency = 0.20\n",
    "                relative_improvement = ((efficiency - baseline_efficiency) / baseline_efficiency) * 100\n",
    "                \n",
    "                all_tech_results.append({\n",
    "                    'climate_scenario': climate_scenario_id,\n",
    "                    'ssp': ssp,\n",
    "                    'climate_quantile': q_label,\n",
    "                    'technology': tech_name,\n",
    "                    'efficiency_%': efficiency * 100,\n",
    "                    'efficiency_improvement_%': relative_improvement,\n",
    "                    'annual_energy_kWh': annual_energy,\n",
    "                    'capacity_factor_%': capacity_factor,\n",
    "                    'mean_power_kW': power_kw.mean(),\n",
    "                    'peak_power_kW': power_kw.max()\n",
    "                })\n",
    "    \n",
    "    tech_results_df = pd.DataFrame(all_tech_results)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TECHNOLOGY SENSITIVITY SUMMARY\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    tech_summary = tech_results_df.groupby('technology').agg({\n",
    "        'efficiency_%': 'first',\n",
    "        'efficiency_improvement_%': 'first',\n",
    "        'annual_energy_kWh': 'mean',\n",
    "        'capacity_factor_%': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Add energy gain metrics\n",
    "    baseline_energy = tech_summary.loc['Current_2025', 'annual_energy_kWh']\n",
    "    tech_summary['energy_gain_kWh'] = (tech_summary['annual_energy_kWh'] - baseline_energy).round(0)\n",
    "    tech_summary['energy_gain_%'] = ((tech_summary['annual_energy_kWh'] / baseline_energy - 1) * 100).round(2)\n",
    "    \n",
    "    print(\"Average Performance Across All Climate Scenarios:\")\n",
    "    print(tech_summary)\n",
    "    print()\n",
    "    \n",
    "    # Save results\n",
    "    output_file = OUTPUT_DIR / 'step5_technology_sensitivity_detailed.csv'\n",
    "    tech_results_df.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Saved detailed results: {output_file}\")\n",
    "    \n",
    "    output_file_summary = OUTPUT_DIR / 'step5_technology_sensitivity_summary.csv'\n",
    "    tech_summary.to_csv(output_file_summary)\n",
    "    print(f\"✓ Saved summary: {output_file_summary}\\n\")\n",
    "    \n",
    "    # Generate comparison analysis\n",
    "    create_technology_climate_comparison(tech_results_df)\n",
    "    \n",
    "    return tech_results_df, tech_summary\n",
    "\n",
    "def create_technology_climate_comparison(tech_results_df):\n",
    "    \"\"\"\n",
    "    Compare relative importance of climate vs technology uncertainty.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tech_results_df : pd.DataFrame\n",
    "        Detailed technology sensitivity results\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"CLIMATE vs TECHNOLOGY IMPACT COMPARISON\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # 1. Climate impact (fixed technology = baseline)\n",
    "    baseline_tech = tech_results_df[tech_results_df['technology'] == 'Current_2025']\n",
    "    climate_variance = baseline_tech.groupby('ssp')['annual_energy_kWh'].agg(['min', 'max', 'mean'])\n",
    "    climate_variance['range_%'] = ((climate_variance['max'] - climate_variance['min']) / climate_variance['mean'] * 100).round(2)\n",
    "    \n",
    "    print(\"Climate Impact Analysis (Fixed Technology = Current 2025):\")\n",
    "    print(climate_variance)\n",
    "    print(f\"\\nAverage climate-induced variation: ±{climate_variance['range_%'].mean():.2f}%\")\n",
    "    \n",
    "    # 2. Technology impact (averaged across climates)\n",
    "    tech_variance = tech_results_df.groupby('technology')['annual_energy_kWh'].mean()\n",
    "    tech_range = ((tech_variance.max() - tech_variance.min()) / tech_variance.min() * 100)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"Technology Impact Range: {tech_range:.2f}%\")\n",
    "    print(f\"  From {tech_variance.min():.0f} kWh/year (Current)\")\n",
    "    print(f\"  To {tech_variance.max():.0f} kWh/year (Future)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # 3. Relative importance\n",
    "    avg_climate_range = climate_variance['range_%'].mean()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"KEY FINDING: RELATIVE IMPORTANCE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Climate uncertainty range: ±{avg_climate_range:.2f}%\")\n",
    "    print(f\"Technology improvement potential: +{tech_range:.2f}%\")\n",
    "    \n",
    "    if tech_range > avg_climate_range * 2:\n",
    "        conclusion = \"Technology improvements have MUCH GREATER impact than climate uncertainty\"\n",
    "    elif tech_range > avg_climate_range:\n",
    "        conclusion = \"Technology improvements have GREATER impact than climate uncertainty\"\n",
    "    else:\n",
    "        conclusion = \"Climate uncertainty is COMPARABLE TO or LARGER than technology impact\"\n",
    "    \n",
    "    print(f\"\\nConclusion: {conclusion}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_data = {\n",
    "        'Factor': ['Climate Variation (Current Tech)', 'Technology Improvement (All Climates)'],\n",
    "        'Impact_%': [avg_climate_range, tech_range],\n",
    "        'Type': ['Uncertainty', 'Opportunity']\n",
    "    }\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    output_file = OUTPUT_DIR / 'step5_climate_vs_technology_comparison.csv'\n",
    "    comparison_df.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Saved comparison: {output_file}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_summary_statistics(df, pv_config=PV_CONFIG):\n",
    "    \"\"\"\n",
    "    Generate annual and monthly summary statistics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Scenarios with PV power output\n",
    "    pv_config : dict\n",
    "        PV system configuration\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    annual_summary : pd.DataFrame\n",
    "        Annual statistics by SSP and quantile\n",
    "    monthly_summary : pd.DataFrame\n",
    "        Monthly statistics by SSP and quantile\n",
    "    \"\"\"\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"STEP 6: GENERATING SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Annual summary\n",
    "    annual_summary = df.groupby(['ssp', 'quantile_label']).agg({\n",
    "        'pv_power_kw': ['sum', 'mean', 'max'],\n",
    "        'poa_global': 'mean',\n",
    "        'temperature_C': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    annual_summary.columns = ['Annual_kWh', 'Mean_Power_kW', 'Peak_Power_kW',\n",
    "                              'Mean_POA_W/m2', 'Mean_Temp_C']\n",
    "    \n",
    "    rated_capacity = pv_config['N_PV'] * pv_config['module_specs']['ratedpowerkw']\n",
    "    annual_summary['Capacity_Factor_%'] = (\n",
    "        (annual_summary['Annual_kWh'] / (rated_capacity * 8760)) * 100\n",
    "    ).round(2)\n",
    "    \n",
    "    print(annual_summary)\n",
    "    \n",
    "    output_file = OUTPUT_DIR / 'step6_annual_summary_statistics.csv'\n",
    "    annual_summary.to_csv(output_file)\n",
    "    print(f\"\\n✓ Saved: {output_file}\\n\")\n",
    "    \n",
    "    # Monthly summary\n",
    "    df['month_name'] = pd.to_datetime(df['datetime']).dt.strftime('%B')\n",
    "    monthly_summary = df.groupby(['ssp', 'quantile_label', 'month', 'month_name']).agg({\n",
    "        'pv_power_kw': ['sum', 'mean'],\n",
    "        'poa_global': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    output_file = OUTPUT_DIR / 'step6_monthly_summary_statistics.csv'\n",
    "    monthly_summary.to_csv(output_file)\n",
    "    print(f\"✓ Saved: {output_file}\\n\")\n",
    "    \n",
    "    return annual_summary, monthly_summary\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Run complete pipeline: SSP data → PV scenarios → Technology sensitivity.\n",
    "    \n",
    "    This pipeline requires:\n",
    "    - SSP climate projection data (CSV files with rsds, tas columns)\n",
    "    - Site configuration (lat/lon, altitude, timezone)\n",
    "    - PV system configuration (tilt, azimuth, module specs)\n",
    "    \"\"\"\n",
    "\n",
    "    # USER MUST SPECIFY: Path to SSP climate data directory\n",
    "    data_path = './climate_data'  # Modify this path \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLIMATE SCENARIOS → PV POWER PIPELINE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Load SSP climate data\n",
    "    df_climate = load_ssp_climate_data(data_path)\n",
    "    \n",
    "    # Step 2: Convert to tilted irradiance\n",
    "    df_with_poa = convert_to_tilted_irradiance_pvlib(df_climate)\n",
    "    \n",
    "    # Step 3: Generate scenarios using KDE\n",
    "    df_scenarios = generate_climate_scenarios(\n",
    "        df_with_poa,\n",
    "        n_synthetic_per_ssp=100,\n",
    "        quantiles=[0.1, 0.5, 0.9]\n",
    "    )\n",
    "    \n",
    "    # Step 4: Calculate PV power\n",
    "    df_with_power = calculate_pv_power_for_scenarios(df_scenarios)\n",
    "    \n",
    "    # Step 5: Technology sensitivity analysis\n",
    "    tech_results, tech_summary = technology_sensitivity_analysis(df_scenarios)\n",
    "    \n",
    "    # Step 6: Summary statistics\n",
    "    annual_summary, monthly_summary = generate_summary_statistics(df_with_power)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  1. step1_raw_climate_data.csv\")\n",
    "    print(\"  2. step2_climate_with_poa_irradiance.csv\")\n",
    "    print(\"  3. step3_climate_scenarios_kde_quantile.csv\")\n",
    "    print(\"  4. step4_scenarios_with_pv_power.csv\")\n",
    "    print(\"  5. step5_technology_sensitivity_detailed.csv\")\n",
    "    print(\"  6. step5_technology_sensitivity_summary.csv\")\n",
    "    print(\"  7. step5_climate_vs_technology_comparison.csv\")\n",
    "    print(\"  8. step6_annual_summary_statistics.csv\")\n",
    "    print(\"  9. step6_monthly_summary_statistics.csv\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
